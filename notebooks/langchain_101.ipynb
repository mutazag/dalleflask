{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4_endpoint: https://lnc-eastus-openai.openai.azure.com/\n",
      "gpt4_key: **********\n",
      "gpt4_region: eastus\n",
      "gpt4_modelid_4k: gpt-4-0314\n",
      "gpt4_modelid_32k: gpt-4-32k-0314\n",
      "gpt4_api_version: 2023-03-15-preview\n",
      "chatgpt_api_key: **********\n",
      "chatgpt_region: southcentralus\n",
      "chatgpt_endpoint: https://magopenai.openai.azure.com/\n",
      "oai_api_key: **********\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import os\n",
    "\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "# get above env variables using os.environ.get and use provided values as defaults\n",
    "gpt4_endpoint = os.environ.get('gpt4_endpoint', 'https://lnc-eastus-openai.openai.azure.com/')\n",
    "gpt4_key = os.environ.get('gpt4_key', '121221')\n",
    "gpt4_region = os.environ.get('gpt4_region', 'eastus')\n",
    "gpt4_modelid_4k = os.environ.get('gpt4_modelid_4k', 'gpt-4-0314')\n",
    "gpt4_modelid_32k = os.environ.get('gpt4_modelid_32k', 'gpt-4-32k-0314')\n",
    "gpt4_api_version = os.environ.get('gpt4_api_version', '2023-03-15-preview')\n",
    "\n",
    "\n",
    "# get the above from os.environ.get and use provided values as defaults\n",
    "chatgpt_api_key = os.environ.get('chatgpt_api_key', '121212121212')\n",
    "chatgpt_region = os.environ.get('chatgpt_region', 'southcentralus')\n",
    "chatgpt_endpoint = os.environ.get('chatgpt_endpoint', 'https://magopenai.openai.azure.com/')\n",
    "\n",
    "oai_api_key = os.environ.get('oai_api_key', '121212121212')\n",
    "\n",
    "print(f'gpt4_endpoint: {gpt4_endpoint}')\n",
    "print(f'gpt4_key: **********')\n",
    "print(f'gpt4_region: {gpt4_region}')\n",
    "print(f'gpt4_modelid_4k: {gpt4_modelid_4k}')\n",
    "print(f'gpt4_modelid_32k: {gpt4_modelid_32k}')\n",
    "print(f'gpt4_api_version: {gpt4_api_version}')\n",
    "print(f'chatgpt_api_key: **********')\n",
    "print(f'chatgpt_region: {chatgpt_region}')\n",
    "print(f'chatgpt_endpoint: {chatgpt_endpoint}')\n",
    "print(f'oai_api_key: **********')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic text completion with a language model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using OpenAI model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI, AzureOpenAI\n",
    "\n",
    "#https://python.langchain.com/en/latest/reference/modules/llms.html#langchain.llms.AzureOpenAI\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = oai_api_key\n",
    "llm = OpenAI(openai_api_key=oai_api_key, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTuesday.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What comes after Monday? \"\n",
    "llm(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Azure OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base = chatgpt_endpoint\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "azure_llm = AzureOpenAI(\n",
    "    deployment_name='text-davinci-003', \n",
    "    model_name='text-davinci-003', \n",
    "    openai_api_key=chatgpt_api_key,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTuesday'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azure_llm(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "template = \"Question: {question}\\n\\n\"\n",
    "\"Answer:\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"],)\n",
    "\n",
    "question = \"once upon a time, in a galaxy far far away ... \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=azure_llm,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: once upon a time, in a galaxy far far away ... \n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'once upon a time, in a galaxy far far away ... ', 'text': '\\nA: There lived a brave and noble hero who was determined to save the galaxy from evil forces. He was a brave knight and a loyal warrior, and his mission was to defeat the dark forces and restore peace and justice to the galaxy. He set off on a quest to find the source of the evil, and with the help of his loyal allies, he succeeded in his mission. The galaxy was once again safe, and the hero returned home, where he was celebrated as a hero.'}\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking Multiple Questions with LLM chain and generate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [\n",
    "    {'question': \"What is the capital of Australia?\"},\n",
    "    {'question': \"Monday, Tuesday, ... ?\"},\n",
    "    {'question': \"what day of week is 33 of December 2019?\"},\n",
    "    {'question': \"is 33 of December 2019 a correct date?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is the capital of Australia?\n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Monday, Tuesday, ... ?\n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: what day of week is 33 of December 2019?\n",
      "\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: is 33 of December 2019 a correct date?\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = llm_chain.generate(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The capital of Australia is Canberra.\n",
      "\n",
      "\n",
      "Answer: Wednesday, Thursday, Friday.\n",
      "\n",
      "Answer: Thursday.\n",
      "\n",
      "\n",
      "Answer: No, December 33 does not exist. The correct date would be December 31.\n"
     ]
    }
   ],
   "source": [
    "for r in res.generations:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chat Scenario \n",
    "#https://blog.langchain.dev/chat-models/ \n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI, ChatAzureOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_dalle_flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
